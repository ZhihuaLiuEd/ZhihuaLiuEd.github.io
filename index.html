<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhihua Liu</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!--<link rel="icon" type="image/png" href="images/seal_icon.png">-->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <!-- <td style="padding:2.5%;width:63%;vertical-align:middle"> -->
              <td style="padding:2%;width:60%;vertical-align:middle;    text-align: justify;
                  text-justify: inter-word;">
              <p style="text-align:center">
                <name>Zhihua Liu</name>
              </p>
              <p>
                Greetings/Γειά σου/泥嚎! My name is Zhihua Liu (Chinese: 刘志华; Greek: Stavros). I am a <a href="https://www.chai.ac.uk/">CHAI</a> Postdoctoral Scholar at the University of Edinburgh, School of Engineering, working with <a href="https://eng.ed.ac.uk/about/people/professor-sotirios-tsaftaris">Prof. Sotirios Tsaftaris</a>.
              </p>
              <p>
                I obtained my Ph.D. from University of Leicester. I was really fortunate to spend wonderful time at AstraZeneca Cambridge Center for AI, as research intern working with <a href="https://chenjin.netlify.app/">Dr. Chen Jin</a> and <a href="https://www.linkedin.com/in/amrutha-saseendran/?originalSubdomain=uk">Dr. Amrutha Saseendran</a>.
              </p>
              <p>
                Previously, I served as an algorithm engineer in JD Logistics, JD.com. I received my M.Sc. in Artificial Intelligence from the University of Edinburgh in 2016, my B.Eng. in Internet of Things from University of Science and Technology Beijing in 2015. I spent my undergraduate final year at the School of Computing in University of Dundee, supervised by <a href="http://staff.computing.dundee.ac.uk/stephen/">Prof. Stephen McKenna</a>, <a href="https://www.gla.ac.uk/schools/computing/staff/sebastianstein/">Dr. Sebastian Stein</a> and <a href="https://faculty.sustech.edu.cn/zhangjg/">Prof. Jianguo Zhang</a>.
              </p>
              <p style="text-align:center">
                <a href="data/Zhihua Liu_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.co.uk/citations?user=je2KXVYAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/Zhihua_L">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/ZhihuaLiuEd">Github</a>
              </p>
              <!--<p>Email: zhliu&lt;dot&gt;ustb&lt;dot&gt;cn at gmail&lt;dot&gt;com</p>-->
              <p>Email: zliu7 at ed&lt;dot&gt;ac&lt;dot&gt;uk</p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%;text-align:center;">
              <a href="images/Zhihua_and_Duoduo.png"><figure></figure><img style="width:100%;max-width:100%" alt="profile photo" src="images/Zhihua_and_Duoduo_Circle.png" class="hoverZoomLink"><figcaption>Zhihua (in black T-shirt) and Duoduo (pure white fur)</figcaption></figure></a>
              <!--<br>-->
              <!--<font>Dundee CS Lab, 2014</font>-->
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research is focusing on medical image analysis, computer vision and machine learning, with recent interests in causal representation learning for visual generation, composition and dynamic perception.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <!--
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Segment_Anyword.png" alt="Boundary_png" style="border-style: none">
            </td>
            -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Segment_Anyword.png" alt="Boundary_png" style="border-style: none; width:160px; height:160px; object-fit: cover;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Segment Anyword: Mask Prompt Inversion for Open-Set Grounded Segmentation</papertitle>
              </a>
              <br>
              <strong>Zhihua Liu</strong>,
              <a href="https://www.linkedin.com/in/amrutha-saseendran/?originalSubdomain=uk">Amrutha Saseendran</a>,
              <a href="https://leitong02.github.io/">Lei Tong</a>,
              <a href="https://c0notsilly.github.io/C0.github.io/">Xilin He</a>,
              <a href="https://scholar.google.co.uk/citations?user=rF9GU-EAAAAJ&hl=en">Fariba Yousefi</a>,
              <a href="https://scholar.google.co.kr/citations?hl=en&user=7h35UZUAAAAJ&view_op=list_works&sortby=pubdate">Nikolay Burlutskiy</a>,
              <a href="https://doglic.bitbucket.io/">Dino Oglic</a>,
              <a href="https://tomdiethe.com/">Tom Diethe</a>,
              <a href="https://scholar.google.com/citations?user=dIFk1scAAAAJ&hl=en">Philip Teare</a>,
              <a href="https://www2.le.ac.uk/departments/informatics/people/huiyu-zhou">Huiyu Zhou</a>,
              <a href="https://chenjin.netlify.app/">Chen Jin</a>
              <br>
              <b>ICML</b>&nbsp;<em>International Conference on Machine Learning</em>, 2025 
              <br>
              <a href="https://arxiv.org/abs/2505.17994">arXiv</a>
              /
              Project Page
              <p></p>
              <p>We introduce Segment Anyword, a training-free visual prompt learning framework with test-time inversed adaption for open-set language grounded segmentation, where visual prompts are simultaneously regularized by linguistic structual information.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/BOTM.png" alt="Boundary_png" style="border-style: none; width:160px; height:160px; object-fit: cover;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!--<a href="https://www.sciencedirect.com/science/article/pii/S136184152400063X">-->
              <papertitle>BOTM: Echocardiography Segmentation via Bi-directional Optimal Token Matching</papertitle>
              </a>
              <br>
              <strong>Zhihua Liu</strong>,
              <a href="https://leitong02.github.io/">Lei Tong</a>,
              <a href="https://c0notsilly.github.io/C0.github.io/">Xilin He</a>,
              <a href="https://scholar.google.com/citations?user=HED_458AAAAJ&hl=zh-CN">Che Liu</a>,
              <a href="https://profiles.imperial.ac.uk/r.arcucci">Rossella Arcucci</a>,
              <a href="https://chenjin.netlify.app/">Chen Jin</a>,
              <a href="https://www2.le.ac.uk/departments/informatics/people/huiyu-zhou">Huiyu Zhou</a>
              <br>
              <b>Pre-print</b>&nbsp;<em>Under Review</em>, 2025 
              <br>
              <a href="https://arxiv.org/abs/2505.18052">arXiv</a>
              /
              Code
              /
              Poster
              <p></p>
              <p>We proposes BOTM, a novel echocardiography segmentation network leveraging visual token anatomical consistency from a transportation perspective.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/LSDM.png" alt="Boundary_png" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.sciencedirect.com/science/article/pii/S136184152400063X">
              <papertitle>LSDM: Long-Short Diffeomorphism Memory Network for Weakly-Supervised Ultrasound Landmark Tracking</papertitle>
              </a>
              <br>
              <strong>Zhihua Liu</strong>,
              <a href="https://www.linkedin.com/in/bin-yang-1518a640/?originalSubdomain=uk">Bin Yang</a>,
              <a>Yan Shen</a>,
              <a>Xuejun Ni</a>,
              <a href="https://www.eng.ed.ac.uk/about/people/dr-sotirios-tsaftaris">Sotirios Tsaftaris</a>,
              <a href="https://www2.le.ac.uk/departments/informatics/people/huiyu-zhou">Huiyu Zhou</a>
              <br>
              <b>MedIA</b>&nbsp;<em>Medical Image Analysis</em>, 2024 
              <br>
              <a href="https://arxiv.org/pdf/2301.04748.pdf">arXiv</a>
              /
              <a href="https://github.com/ZhihuaLiuEd/LSDM">Code</a>
              /
              <a href="posters/Zhihua Liu LSDM.pdf">Poster</a>
              <p></p>
              <p>Accurate tracking of an anatomical landmark over time has been of high interests for disease assessment such as minimally invasive surgery and tumor radiation therapy. In this paper, we propose a long-short diffeomorphic motion network, which is a multi-task framework with a learnable deformation prior to search for the plausible deformation of landmark.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/braintumorsurvey2.png" alt="Boundary_png" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://link.springer.com/article/10.1007/s40747-022-00815-5">
              <papertitle>Deep Learning Based Brain Tumor Segmentation: A Survey</papertitle>
              </a>
              <br>
              <strong>Zhihua Liu</strong>,
              <a href="https://leitong02.github.io/">Lei Tong</a>,
              <a>Zheheng Jiang</a>,
              <a>Long Chen</a>,
              <a>Feixiang Zhou</a>,
              <a href="http://eecs.qmul.ac.uk/profiles/zhangqianni.html">Qianni Zhang</a>,
              <a href="https://scholar.google.co.uk/citations?user=G6AdRfwAAAAJ&hl=en">Xiangrong Zhang</a>,
              <a href="https://scholar.google.co.uk/citations?user=B5WAkz4AAAAJ&hl=en">Yaochu Jin</a>,
              <a href="https://www2.le.ac.uk/departments/informatics/people/huiyu-zhou">Huiyu Zhou</a>
              <br>
              <b>CAIS</b>&nbsp;<em>Complex & Intelligent Systems</em>, 2022 
              <br>
              <a href="https://arxiv.org/pdf/2007.09479.pdf">arXiv</a>
              /
              <a href="https://github.com/ZhihuaLiuEd/SoTA-Brain-Tumor-Segmentation">code</a>
              <p></p>
              <p> Considering stateof-the-art technologies and their performance, the purpose of this paper is to provide a comprehensive survey of recently developed deep learning based brain tumor segmentation techniques.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/twitterdepression.png" alt="Boundary_png" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9691852">
              <papertitle>Cost-sensitive Boosting Pruning Trees for depression detection on Twitter</papertitle>
              </a>
              <br>
              <a href="https://leitong02.github.io/">Lei Tong</a>,
              <strong>Zhihua Liu</strong>,
              <a>Zheheng Jiang</a>,
              <a>Feixiang Zhou</a>,
              <a>Long Chen</a>,
              <a>Jialin Lyu</a>,
              <a>Xiangrong Zhang</a>,
              <a href="http://eecs.qmul.ac.uk/profiles/zhangqianni.html">Qianni Zhang</a>,
              <a href="https://www.brunel.ac.uk/people/abdul-h-sadka/publications">Abdul Sadka</a>,
              <a href="https://scholar.google.com/citations?user=WNY0TscAAAAJ&hl=en">Yinhai Wang</a>,
              <a href="https://www.kent.ac.uk/computing/people/3061/li-caroline">Ling Li</a>,
              <a href="https://www2.le.ac.uk/departments/informatics/people/huiyu-zhou">Huiyu Zhou</a>
              <br>
              <b>IEEE TAE</b>&nbsp;<em>IEEE Trans. on Affective Computing</em>, 2022 
              <br>
              <a href="https://arxiv.org/pdf/1906.00398.pdf">arXiv</a>
              /
              <a href="https://github.com/BIPL-UoL/Cost-Boosting-Pruning-Trees-for-depression-detection-on-Twitter">code</a>
              /
              <a href="https://www.independent.co.uk/tech/twitter-bot-depressed-user-profiles-b2052664.html">Media</a>
              <p></p>
              <p>A novel classifier, namely, Cost-sensitive Boosting Pruning Trees (CBPT), which demonstrates a strong classification ability on two publicly accessible Twitter depression detection datasets..</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/canet.png" alt="Boundary_png" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9378564">
              <papertitle>CANet: Context Aware Network for Brain Glioma Segmentation</papertitle>
              </a>
              <br>
              <strong>Zhihua Liu</strong>,
              <a href="https://leitong02.github.io/">Lei Tong</a>,
              <a>Long Chen</a>,
              <a>Feixiang Zhou</a>,
              <a>Zheheng Jiang</a>,
              <a href="http://eecs.qmul.ac.uk/profiles/zhangqianni.html">Qianni Zhang</a>,
              <a href="https://scholar.google.com/citations?user=WNY0TscAAAAJ&hl=en">Yinhai Wang</a>,
              <a href="https://sites.google.com/site/caifengshan/">Caifeng Shan</a>,
              <a href="https://www.kent.ac.uk/computing/people/3061/li-caroline">Ling Li</a>,
              <a href="https://www2.le.ac.uk/departments/informatics/people/huiyu-zhou">Huiyu Zhou</a>
              <br>
              <b>IEEE TMI</b>&nbsp;<em>IEEE Trans. on Medical Imaging</em>, 2021 
              <br>
              <a href="https://arxiv.org/pdf/2007.07788.pdf">arXiv</a>
              /
              <a href="https://github.com/ZhihuaLiuEd/canetbrats">code</a>
              <p></p>
              <p>A novel approach named Context-Aware Network (CANet) for brain glioma segmentation.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/MPhilThesis3.png" alt="Boundary_png" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2303.16099">
              <papertitle>MPhil Thesis: Medical Image Analysis using Deep Relational Learning</papertitle>
              </a>
              <br>
              <strong>Zhihua Liu</strong>
              <br>
              <em>MPhil Thesis</em>, 2020 
              <br>
              <a href="https://arxiv.org/abs/2303.16099">arXiv</a>
              /
              <a href="posters/Zhihua Liu PG Research Day.pdf">Poster</a>
              <p></p>
              <p>Benefited from deep learning techniques, remarkable progress has been made within the medical image analysis area in recent years. However, it is very challenging to fully utilize the relational information (the relationship between tissues or organs or images) within the deep neural network architecture. Thus in this thesis, we propose two novel solutions to this problem called implicit and explicit deep relational learning. We generalize these two paradigms of deep relational learning into different solutions and evaluate them on various medical image analysis tasks.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/BEngThesis3.png" alt="Boundary_png" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/17lfkv19P5LtdJkK5lPTrNrQNPgTJJNE-/view?usp=sharing">
              <papertitle>BEng Thesis: Multi-Classes Training and Testing in Food Preparation Recognition Tasks</papertitle>
              </a>
              <br>
              <strong>Zhihua Liu</strong>
              <br>
              <em>BEng Thesis</em>, 2015 
              <br>
              <a href="https://drive.google.com/file/d/17lfkv19P5LtdJkK5lPTrNrQNPgTJJNE-/view?usp=sharing">PDF</a>
              /
              <a href="posters/Zhihua Liu BEng.pdf">Poster</a>
              /
              <a href="https://cvip.computing.dundee.ac.uk/datasets/foodpreparation/50salads/">Dataset</a>
              <p></p>
              <p>The recognition of human action is widely applied in video surveillance, virtual reality and in some human-computer interaction areas such as user experience designing tasks. Pattern recognition becomes a hot topic in the field of computer vision. In this report, I summarize human behavior recognition problem as a problem of acquiring computing data through motion detection and symbolic acting information. Then extract and understand the behavior of the action features to achieve classification target. On this basis, I review the moving object detection, motion feature extraction and movement characteristics to understand the technical analysis, the correlation method classification, and discuss the difficulties and research directions of this project.</p>
            </td>
          </tr>

        </tbody></table>

        <!-- /
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Projects</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          
          <tr onmouseout="flyspin_stop()" onmouseover="flyspin_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div id='flyspin' class='hidden'><img src="images/Mosaicing2.gif"></div>
              <div id='flystill'>
                <a href="images/Mosaicing.gif"><img src="images/Mosaicingbegin.png"></a>
              </div>
              <script type="text/javascript">
                function flyspin_start() {
                  document.getElementById('flyspin').style.display = 'inline';
                  document.getElementById('flystill').style.display = 'none';
                }

                function flyspin_stop() {
                  document.getElementById('flyspin').style.display = 'none';
                  document.getElementById('flystill').style.display = 'inline';
                }
                flyspin_stop()
              </script>
            </td>
            <td width="75%" valign="center">
              <a>3D Reconstruction and Video Mosaicking with Applications to Fetoscopy</a>
              <br>
              <br>
              <font>July 2020</font>
              <br>
              <br>
              <font>Group project during UCL Medical Image Computing Summer School (MedICSS) 2020.</font>
              <br>
              <br>
              <a href="https://drive.google.com/file/d/1oJa2dJFrfyHq2mTDQ2JJryU2ItkJlMap/view?usp=sharing">Presentation Slide</a>
              /
              <a href="https://arxiv.org/pdf/1907.06543.pdf">Related Paper</a>
              /
              <a href="https://www.ucl.ac.uk/interventional-surgical-sciences/fetoscopy-placenta-data">Related Dataset</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/pointcloudjd.png"></td>
            <td width="75%" valign="center">
              <a>Object detection in 3D point cloud</a>
              <br>
              <br>
              <font>Jan 2017-Sep 2018</font>
              <br>
              <br>
              <font>Industrial research and engineering work at</font> <a href="https://www.jdl.cn/">JD Logistics, JD.com.</a><font>Focused on vehicle, bicycle and pedestrian detection from 3D point cloud generated from LiDAR. Also focused on engineering work within point cloud data storge, data retrival, data access authentication development.</font>
            </td>
          </tr>
        </tbody></table>
      -->

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching Assistant</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              <font color="#FF0000">2020-2023</font>
              <br>
              <font>CO1104 Computer Architecture</font>
              <br>
              <font>CO4105 Advanced C++ Programming</font>
              <br>
              <font>CO3002 Analysis and Design of Algorithms</font>
              <br>
              <font color="#FF0000">2019-2020</font> 
              <br>
              <font>FS0023 STEM Foundation Year Lab-Physics</font> 
            </td>
          </tr>
        </tbody></table>

        <!-- /
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Blog Posts</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              <font color="#FF0000">Class A</font>
              <br>
              <font><a href="posts/test.md">test</font>
              <br>
              <br>
              <font color="#FF0000">Class B</font>
            </td>
          </tr>
        </tbody></table>
        -->

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Others</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              <font>I like traveling and photography. Here is my <a href="https://www.instagram.com/zhihua_liu_ed/">instagram</a>.</font>
              <br>
              <br>
              <font>I also like sports, especially football &#9917; and table tennis &#127955;. I started to receive professional table tennis training from the age of 5, got into the school team, and gave up training in high school because of the college entrance examination.</font>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <font>This <a href="https://github.com/jonbarron/website">website template</a> is from <a href="https://jonbarron.info/">Jon Barron</a>.<font> 
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
